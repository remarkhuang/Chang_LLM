# Free LLM Gateway

ç»Ÿä¸€æ¥å£è°ƒç”¨å¤šç§å…è´¹LLMåç«¯çš„é¡¹ç›®ï¼Œæ”¯æŒ Ollamaã€vLLMã€LM Studio å’Œ Groqã€‚

## é¡¹ç›®ç»“æ„

```
free_LLM/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py          # FastAPI ä¸»åº”ç”¨
â”‚   â”œâ”€â”€ config.py        # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ llm_client.py    # LLM å®¢æˆ·ç«¯å®ç°
â”‚   â”œâ”€â”€ requirements.txt # Python ä¾èµ–
â”‚   â””â”€â”€ .env.example     # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html       # Web å‰ç«¯ç•Œé¢
â”œâ”€â”€ start.bat            # Windows å¯åŠ¨è„šæœ¬
â”œâ”€â”€ start.sh             # Linux/Mac å¯åŠ¨è„šæœ¬
â””â”€â”€ README.md
```

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd backend
pip install -r requirements.txt
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

```bash
cp backend/.env.example backend/.env
# ç¼–è¾‘ .env æ–‡ä»¶é…ç½®ä½ çš„ API keys
```

### 3. å¯åŠ¨æœåŠ¡

**Windows:**
```bash
start.bat
```

**Linux/Mac:**
```bash
chmod +x start.sh
./start.sh
```

æˆ–æ‰‹åŠ¨å¯åŠ¨ï¼š
```bash
# åç«¯
cd backend
python -m uvicorn main:app --reload --port 8000

# å‰ç«¯
cd frontend
python -m http.server 3000
```

### 4. è®¿é—®åº”ç”¨

- å‰ç«¯ç•Œé¢: http://localhost:3000
- API æ–‡æ¡£: http://localhost:8000/docs

---

## LLM æä¾›å•†é…ç½®è¯¦è§£

### ğŸ¦™ Ollama (æ¨èæœ¬åœ°ä½¿ç”¨)

**ç‰¹ç‚¹:**
- å®Œå…¨å…è´¹ï¼Œæœ¬åœ°è¿è¡Œ
- æ”¯æŒå¤šç§å¼€æºæ¨¡å‹
- éšç§å®‰å…¨ï¼Œæ•°æ®ä¸å‡ºæœ¬åœ°
- å®‰è£…ç®€å•ï¼Œå¼€ç®±å³ç”¨

**å®‰è£…:**
```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# è®¿é—® https://ollama.com/download ä¸‹è½½å®‰è£…
```

**ä½¿ç”¨:**
```bash
# ä¸‹è½½å¹¶è¿è¡Œæ¨¡å‹
ollama run llama2
ollama run mistral
ollama run codellama

# æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
ollama list

# API è°ƒç”¨
curl http://localhost:11434/api/chat -d '{
  "model": "llama2",
  "messages": [{"role": "user", "content": "Hello!"}]
}'
```

**Python è°ƒç”¨:**
```python
import httpx

response = httpx.post("http://localhost:11434/api/chat", json={
    "model": "llama2",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": False
})
print(response.json()["message"]["content"])
```

---

### âš¡ vLLM (é«˜æ€§èƒ½æ¨ç†)

**ç‰¹ç‚¹:**
- é«˜ååé‡ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ
- éœ€è¦ GPU æ”¯æŒ
- OpenAI å…¼å®¹ API
- æ”¯æŒ PagedAttention ä¼˜åŒ–

**å®‰è£…:**
```bash
pip install vllm
```

**å¯åŠ¨æœåŠ¡:**
```bash
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --host 0.0.0.0 \
    --port 8000
```

**Python è°ƒç”¨:**
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```

---

### ğŸ–¥ï¸ LM Studio (å›¾å½¢ç•Œé¢)

**ç‰¹ç‚¹:**
- å›¾å½¢ç•Œé¢ï¼Œæ˜“äºä½¿ç”¨
- æ”¯æŒä» HuggingFace ä¸‹è½½æ¨¡å‹
- è‡ªåŠ¨æä¾› OpenAI å…¼å®¹ API
- é€‚åˆéæŠ€æœ¯ç”¨æˆ·

**å®‰è£…:**
1. è®¿é—® https://lmstudio.ai ä¸‹è½½
2. å®‰è£…åæ‰“å¼€åº”ç”¨
3. æœç´¢å¹¶ä¸‹è½½æ¨¡å‹ (å¦‚ Llama 2, Mistral)
4. å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ (é»˜è®¤ç«¯å£ 1234)

**API è°ƒç”¨:**
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio"
)

response = client.chat.completions.create(
    model="local-model",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

---

### ğŸš€ Groq (äº‘ç«¯å…è´¹)

**ç‰¹ç‚¹:**
- äº‘ç«¯æœåŠ¡ï¼Œæ— éœ€æœ¬åœ°èµ„æº
- è¶…å¿«æ¨ç†é€Ÿåº¦ (LPU èŠ¯ç‰‡)
- æœ‰å…è´¹é¢åº¦
- æ”¯æŒå¤§æ¨¡å‹ (Llama 2 70B, Mixtral)

**æ³¨å†Œ:**
1. è®¿é—® https://console.groq.com
2. æ³¨å†Œè´¦å·è·å– API Key

**é…ç½®:**
```bash
export GROQ_API_KEY=your_api_key
```

**API è°ƒç”¨:**
```python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key="your_api_key"
)

response = client.chat.completions.create(
    model="llama2-70b-4096",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

**å¯ç”¨æ¨¡å‹:**
- `llama2-70b-4096` - Llama 2 70B
- `mixtral-8x7b-32768` - Mixtral 8x7B
- `gemma-7b-it` - Gemma 7B

---

## API æ¥å£è¯´æ˜

### è·å–æä¾›å•†åˆ—è¡¨
```
GET /providers
```

### è·å–æ¨¡å‹åˆ—è¡¨
```
GET /providers/{provider}/models
```

### å‘é€èŠå¤©è¯·æ±‚
```
POST /chat
{
    "messages": [{"role": "user", "content": "Hello"}],
    "provider": "ollama",
    "model": "llama2",
    "temperature": 0.7,
    "max_tokens": 2048
}
```

### æµå¼èŠå¤©
```
POST /chat/stream
```
è¿”å› Server-Sent Events (SSE) æ ¼å¼çš„æµå¼å“åº”ã€‚

---

## å¸¸è§é—®é¢˜

### Q: Ollama è¿æ¥å¤±è´¥ï¼Ÿ
ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼š
```bash
ollama serve
```

### Q: vLLM å¯åŠ¨å¤±è´¥ï¼Ÿ
ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU æ˜¾å­˜ï¼Œæˆ–å°è¯•é‡åŒ–æ¨¡å‹ï¼š
```bash
python -m vllm.entrypoints.openai.api_server \
    --model TheBloke/Llama-2-7B-GPTQ \
    --quantization gptq
```

### Q: å¦‚ä½•æ·»åŠ æ–°çš„ LLM åç«¯ï¼Ÿ
åœ¨ `llm_client.py` ä¸­æ·»åŠ æ–°çš„å®¢æˆ·ç«¯ç±»ï¼Œç»§æ‰¿ `BaseLLMClient` å¹¶å®ç°ç›¸åº”æ–¹æ³•ã€‚

---

## è®¸å¯è¯

MIT License
